---
title: "Seoul Bike Sharing Demand"
author: "STAT 420, Summer 2022, Scott Downey, Sushma Ponna, William Shih"
output:
  html_document: 
    theme: readable
    toc: yes
urlcolor: cyan
---

***

## Introduction
Rental bikes and bike sharing are important in urban cities as bikes reduce traffic congestion, reduce air pollution, and improve last mile transportation. Bikes will become even more important in the future as cities urbanize and have high population densities. The goal of this project is to build a model that will accurately predict the volume of bike rentals at a given time. Predicting exactly many bikes to provide makes sure that there will be no shortage of bikes that would result in people waiting for a bike yet also not have a large surplus of bikes as that would be a waste of resources.

The data set contains bike sharing data in Seoul from 12/1/2017 to 11/30/2018. Each observation represents an hour of time. The variable of interest for prediction is the rented bike count for the hour. The 12 variables that will be considered as predictors include the hour of day, temperature, humidity, wind speed, visibility, dew point, solar radiation, rainfall, snowfall, season of year, holiday (boolean), and functioning (boolean). The dataset is associated with two papers published in a journal in February and March 2020 related to gradient tree boosting, but here we'll be only using linear regression. and is found at the Seoul government website http://data.seoul.go.kr/

A further description of the dataset can be found at https://archive.ics.uci.edu/ml/datasets/Seoul+Bike+Sharing+Demand

## Methods

We began by importing and cleaning the raw data from the dataset described above.  As part of this, we transformed several predictors into Factor and Integer variables.  We also broke out the 'Date' field into 'Day', 'Month' and 'Year'.  We eventually discarded the 'Year' predictor, as our model will be focused on predicting usage for future years (and this will enable a 'year agnostic' model to predict usage regardless of yearly data).

```{r, message = FALSE, warning = FALSE, fig.height= 15, fig.width = 15}
#Load the bike sharing data into R
require(dplyr)
library(tidyr)
library(ggplot2)
library(car)
colnames = c("Date","RentedBikeCount","Hour","Temperature","Humidity","Wind","Visibility",
             "DewPoint","SolarRadiation","Rainfall","Snowfall","Season","Holiday","Functional")
data = read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/00560/SeoulBikeData.csv",
                col.names = colnames)
data = tibble(data)

#Separate 'Date'  into Day, Month and Year
data = separate(data, "Date", c("Day", "Month", "Year"))

#Convert predictors to factors or integers as needed
data$Season = as.factor(data$Season)
data$Holiday = as.factor(data$Holiday)
data$Functional = as.factor(data$Functional)

data$Day = as.integer(data$Day)
data$Month = as.integer(data$Month)
data$Year = as.integer(data$Year)

#Remove 'Year' as this won't be a good predictor for future years
data = select(data, -Year)

```

We investigated the data, particularly how predictors related to each other through collinearity.  Below is a correlation plot describing each.  Temperature and (to a lesser degree) Humidity both had high correlation to Dewpoint.

```{r, warning = FALSE}
correlations = round(cor(data %>% select(-c(Day, Month, Season, Holiday, Functional))), 2)
cor_melt = reshape2::melt(correlations)
ggplot(cor_melt, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile() +
  theme(axis.text.x = element_text(angle = 90)) +
  xlab("x") + ylab("y") + ggtitle("Correlation Plot") +
  scale_fill_gradient2(low = "#075AFF",
                       mid = "#FFFFCC",
                       high = "#FF0000") +
  geom_label(aes(label = value), color = "black", label.size = NA)
```

We looked at the response 'RentedBikeCount' as it relates to the various predictors we have available.  Below we compared time and date (Day, Month and Hour), as well as environment predictors like temperature, humidity, rainfall, snowfall, solar radiation, etc.

```{r, warning = FALSE}
draw_plot = function(data){
  data %>%
    gather(-RentedBikeCount, key = "xaxis", value = "value") %>%
    ggplot(aes(x = value, y = RentedBikeCount)) + 
    geom_point() + theme_bw() +
    stat_smooth(geom = "smooth", size = 2, level = 0.95) +
    facet_wrap(xaxis ~ ., scales = "free_x")
}
draw_plot(data %>% select(RentedBikeCount, Hour, Day, Month))
```



```{r, warning = FALSE}
draw_plot(data %>% select(RentedBikeCount, Temperature, Humidity, Wind, Visibility) %>%
  rename(`Humidity (%)` = Humidity, `Temperature (C)` = Temperature, `Wind (m/s)` = Wind, `Visibility (10 m)` = Visibility))
```


```{r, message = FALSE}
draw_plot(data %>% select(RentedBikeCount, SolarRadiation, Rainfall, Snowfall) %>%
  rename(`Solar radiation (MJ/m2)` = SolarRadiation, `Rainfall (mm)` = Rainfall, `Snowfall (cm)` = Snowfall))
```


```{r, warning = FALSE}
data %>%
  select(RentedBikeCount, Season, Holiday, Functional) %>%
  rename(`Functional Day` = Functional) %>%
  pivot_longer(., -RentedBikeCount, names_to = "Variable", values_to = "Value") %>%
  ggplot(aes(x = Value, y = RentedBikeCount)) + theme_bw() + 
    geom_boxplot() + 
    facet_wrap(. ~ Variable, scales = "free_x")
```



```{r, warning = FALSE}
get_loocv_rmse = function(model) {
  sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2))
}
get_rmse = function(model){
  actual = bike_tst$RentedBikeCount
  pred = predict(model, bike_tst)
  n = length(pred)
  sqrt(1/n * sum((actual-pred)^2))
}
evaluate = function(models){
  num_coefs = sapply(models, function(x) length(coef(x)))
  loocv_rmse = sapply(models, get_loocv_rmse)
  bics_list = unlist(sapply(models, AIC, k = log(6132)))
  rmse_list = sapply(models, get_rmse)
  R2adj_list = prettyNum(sapply(models, function(x) summary(x)$adj.r.squared), digits = 3, format = "f")
  frame = cbind(num_coefs, loocv_rmse, bics_list, rmse_list)
  frame[,-1] = formatC(frame[,-1], digits = 2, format = "f", big.mark = ",")
  frame = cbind(frame, R2adj_list)
  colnames(frame) = c("Coefficients", "LOOCV-RMSE","BIC","Test RMSE","Adjusted R2")
  return(frame)
}
```

We then split our data into 'Train' and 'Test' data sets.  We used a 70/30 split Train/Test, where we would build our model on the 'Train' data and test its performance on the 'Test' dataset.

```{r, message = FALSE, warning = FALSE, fig.height= 15, fig.width = 15}
#Split data into training and test data
set.seed(42)
trn_size = as.integer(0.7 * count(data))
trn_idx = sample(nrow(data), trn_size)
bike_trn = data[trn_idx, ]
bike_tst = data[-trn_idx, ]
```

We began with several initial models - an multiple linear regressions (MLR) additive model and several interaction models, ending will a "full" interaction model with all two-way interaction between the predictors.  We decided to use three metrics to test the validity of variable subset selection: LOOCV-RMSE, BIC and Test RMSE.

```{r, message = FALSE, warning = FALSE, fig.height= 15, fig.width = 15}
#Initial model starting points
model_add = lm(RentedBikeCount ~ ., data = bike_trn)
model_fact = lm(RentedBikeCount ~ . + factor(Hour), data = bike_trn)
model_fact2 = lm(RentedBikeCount ~ . + factor(Hour) + (Temperature * .), data = bike_trn)
model_fact3 = lm(RentedBikeCount ~ . + factor(Hour) + (Temperature * .) + (Hour*.), data = bike_trn)
model_fact4 = lm(RentedBikeCount ~ . + factor(Hour) + (Temperature * .) + (Humidity*.) + (Hour*.), data = bike_trn)
model_fact5 = lm(RentedBikeCount ~ (.)^2 + factor(Hour), data = bike_trn)
continuous = c("Day","Temperature","Humidity","Wind","Visibility","DewPoint","SolarRadiation","Rainfall","Snowfall")
form_2 = as.formula(
    paste('RentedBikeCount ~ (.)^2 + factor(Hour) + ', paste('I(',continuous,'^2)*.',collapse = ' + ')))
model_fact6 = lm(RentedBikeCount ~ (.)^2 + factor(Hour) + I(Temperature^2)*. + I(Wind^2)*., data = bike_trn)
model_fact7 = lm(form_2, data = bike_trn)
models = list(model_add, model_fact, model_fact2, model_fact3, model_fact4, model_fact5, model_fact6, model_fact7)
```

```{r, warning = FALSE}
frame = evaluate(models)
rownames(frame) = c("Additive","Hour as Factor","Interaction with Temperature",
                    "Interaction with Temperature,Hour","Interaction with Temperature,Humidity,Hour",
                    "All Interaction", "All Interaction (Quadratic for Wind/Temp)", "All Interaction with Quadratics")
frame = formatC(frame, digits = 2, format = "f", big.mark = ",")
knitr::kable(frame, digits = 2, caption = "Evaluation of Models (Test RMSE of XBoost Tree = 183.80)")
```

Once we selected the initial model we liked best based on the metrics above, we ran this through AIC and BIC procedures in an attempt to simplify the model without sacrificing accuracy to a material degree.

```{r, message = FALSE, warning = FALSE, fig.height= 15, fig.width = 15}
aic_model = step(model_fact6, direction = "backward", trace = 0)
bic_model = step(model_fact6, span = RentedBikeCount ~ 1, direction = "backward", k = log(6132), trace = 0)

anova(aic_model, model_fact6)
anova(bic_model, model_fact6)

```
```{r message=FALSE, warning=FALSE}
#Initial model comparison, based on LOOCV-RMSE, BIC and Test RMSE
models = list(model_fact6, aic_model, bic_model)

```

```{r, warning = FALSE}
frame = evaluate(models)
rownames(frame) = c("All Interaction", "AIC Model", "BIC Model")
frame = formatC(frame, digits = 2, format = "f", big.mark = ",")
knitr::kable(frame, digits = 2, caption = "Evaluation of Models (Test RMSE of XBoost Tree = 183.80)")
```

## Results

## Discussion

## Appendix

